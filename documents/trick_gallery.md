# Tricks, corresponding results, experimental settings, and running commands

- The file contains the results, experimental settings, and running commands of different tricks. These tricks are divided into four families, which are re-weighting, re-sampling, mixup training, and two-stage training. For more details of the above four trick families, see the [original paper](https://cs.nju.edu.cn/wujx/paper/AAAI2021_Tricks.pdf).
- For any problem, such as bugs, feel free to open an issue.
- <u>Before you train, you should change the training and validation jsons in CONFIG files to your own path.</u>
- <u>Click each method to get the experimental setting  configs and running commands.</u>

### Re-weighting

- Strictly speaking, the LDAM loss, CrossEntropyLabelSmooth, CDT, and SEQL do not belong to re-weighting methods, but both of them consider the long-tailed distribution when calculate the losses, and they can be combined with re-weighting in DRW. So we add them to re-weighting family.

| Datasets                                                     | CIFAR-10-LT-100 | CIFAR-10-LT-50 | CIFAR-100-LT-100 | CIFAR-100-LT-50 |
| ------------------------------------------------------------ | :-------------: | :------------: | :--------------: | :-------------: |
| <details><summary>Baseline</summary> <ol><li>CONFIG (from left to right):<ul>          <li>config/cui_cifar/baseline/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.12      |     24.81      |      61.76       |      57.65      |
| <details><summary>CE_CE</summary> <ol><li>Introduction: <ul><li>The most commonly used re-weighting method, you can see Eq. (2) in our [paper]() for more details. </li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/csce/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>  </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      31.22      |     22.06      |      69.41       |      62.05      |
| <details><summary>Square CS_CE</summary> <ol><li>Introduction: <ul><li>This is a smooth version of CE_CE (smooth CS_CE), which add a hyper-parameter $ \gamma$ to vanilla CS_CE. In smooth CS_CE, the loss weight of class i is defined as: $(\frac{N_{min}}{N_i})^\gamma$, where $\gamma \in [0, 1]$, $N_i$ is the number of images in class i. We set $\gamma = 0.5$ to get a square-root version of CS_CE (Square CE_CE). </li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/csce/{cifar10_im100_square.yaml, cifar10_im50_square.yaml, cifar100_im100_square.yaml, cifar100_im50_square.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      31.58      |     22.78      |      62.01       |      57.66      |
| <details><summary>Focal loss</summary> <ol><li>Introduction: <ul><li>Focal loss makes the model focus training on difficult samples, and you can see Eq. (4) in our [paper]() for more details.</li><li>The Focal loss paper link: [Lin et al., ICCV 2017](https://arxiv.org/abs/1708.02002).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/focal/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.88      |     24.12      |      62.59       |      59.46      |
| <details><summary>ClassBalanceFocal</summary><ol><li>Introduction: <ul><li>The modified version of Focal loss, which is based on the theory of effective numbers, and you can see Eq. (5) in our [paper]() for more details.</li><li>The ClassBalanceFocal paper link: [Cui et al., CVPR 2019](https://arxiv.org/abs/1901.05555).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/cbfocal/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      26.83      |     21.71      |      62.11       |      57.70      |
| <details><summary>ClassBalanceCE</summary><ol><li>Introduction: <ul><li>The modified version of cross-entropy loss, which is based on the theory of effective numbers, and you can see Eq. (6) in our [paper]() for more details.</li><li>The ClassBalanceCE paper link: [Cui et al., CVPR 2019](https://arxiv.org/abs/1901.05555).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/cbce/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      29.80      |     22.91      |      61.50       |      57.98      |
| <details><summary>CrossEntropyLabelSmooth</summary><ol><li>Introduction: <ul><li>The commonly used regularization trick, label smoothing,  based on cross-entropy loss. </li><li>The CrossEntropyLabelSmooth paper link: [Szegedy et al., CVPR 2016](https://arxiv.org/abs/1512.00567).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/cels/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.44      |     24.35      |      62.80       |      57.55      |
| <details><summary>LDAM loss</summary><ol><li>Introduction: <ul><li>LDAM loss is one of metric learning methods, which aims to assign different margins to different class. </li><li>The LDAM loss paper link: [Cao et al., NeurIPS 2019](https://arxiv.org/abs/1906.07413).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/ldam/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      29.37      |     23.15      |      61.24       |      57.86      |
| <details><summary>SEQL</summary><ol><li>**The SEQL in my implementation doesn't work. The loss will be NaN in training procedure. I will figure out this bug in next few days.**</li><br/><li>Introduction: <ul><li>The softmax equalization loss (SEQL) aims to reduce the gradients of tail classes' negative samples. The author argues that the imbalance of gradients in tail classes' positive and negtive samples causes bad influences.</li><li>The SEQL paper link: [Tan et al., CVPR 2020](https://arxiv.org/abs/2003.05176).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/seql/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |       --        |       --       |        --        |       --        |
| <details><summary>CDT</summary><ol><li>Introduction: <ul><li>The authors find that a model significantly over-fits the tail classes, and they argue that feature deviation between the training and test samples causes this problem. So they propose class-dependent temperatures (CDT). </li><li>The CDT paper link: [Ye et al., arXiv 2020](https://arxiv.org/abs/2001.01385).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/cdt/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      25.08      |   **19.89**    |      59.90       |      56.33      |
| <details><summary>BalancedSoftmaxCE</summary><ol><li>Introduction: <ul><li>A simple and effective re-weighting method, and you can see Eq. (4) in the author paper. </li><li>The BalancedSoftmaxCE paper link: [Ren et al., NeurIPS 2020](https://arxiv.org/abs/2007.10740).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/loss/bsce/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |    **23.63**    |     20.07      |    **56.82**     |    **54.55**    |

### Re-sampling

| Datasets                                                     | CIFAR-10-LT-100 | CIFAR-10-LT-50 | CIFAR-100-LT-100 | CIFAR-100-LT-50 |
| ------------------------------------------------------------ | :-------------: | :------------: | :--------------: | :-------------: |
| <details><summary>Baseline</summary> <ol><li>CONFIG (from left to right):<ul>          <li>config/cui_cifar/baseline/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.12      |     24.81      |      61.76       |      57.65      |
| <details><summary>Class-balanced sampling</summary><ol><li>Introduction: <ul><li>Class-balanced sampling makes each class to have an equal probability of being selected, and you can see the section `Re-sampling` in our [paper]() for more details. </li><li>The class-balanced sampling paper link: [Kang et al., ICLR 2020](https://arxiv.org/abs/1910.09217).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/resampling/balance/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.35      |   **23.38**    |      66.47       |      61.07      |
| <details><summary>Square-root sampling</summary><ol><li>Introduction: <ul><li>Square-root sampling aims to return a lighter imbalanced dataset., and you can see the section `Re-sampling` in our [paper]() for more details. </li><li>The square-root sampling paper link: [Kang et al., ICLR 2020](https://arxiv.org/abs/1910.09217).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/resampling/square/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.81      |     23.61      |      64.67       |      58.03      |
| <details><summary>Progressively-balanced sampling</summary><ol><li>Introduction: <ul><li>Progressively-balanced sampling changes the sampling probabilities of classes from random sampling to class-balanced sampling., and you can see the section `Re-sampling` in our [paper]() for more details. </li><li>The progressively-balanced sampling paper link: [Kang et al., ICLR 2020](https://arxiv.org/abs/1910.09217).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/resampling/progressive/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |    **29.35**    |     24.06      |    **60.90**     |    **56.92**    |

### Mixup training

| Datasets                                                     | CIFAR-10-LT-100 | CIFAR-10-LT-50 | CIFAR-100-LT-100 | CIFAR-100-LT-50 |
| ------------------------------------------------------------ | :-------------: | :------------: | :--------------: | :-------------: |
| <details><summary>Baseline</summary> <ol><li>CONFIG (from left to right):<ul>          <li>config/cui_cifar/baseline/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.12      |     24.81      |      61.76       |      57.65      |
| <details><summary>Baseline (400 epochs)</summary><ol><li>Using baseline settings to train 400 epochs. On CIFAR-LT, the learning rate should be divided by 100 at the 320th and 360th epoch, respectively.</li></ol></details> |      29.71      |     23.50      |      60.92       |      56.59      |
| <details><summary>Input mixup</summary> <ol><li>Introduction: <ul><li>In input mixup, each new example is formed with two randomly sampled example by a weighted linear interpolation, and we only use the new example to train the network. You can see the section `Mixup training` in our [paper]() for more details. </li><li>The mixup paper link: [Zhang et al., ICLR 2018](https://arxiv.org/abs/1710.09412).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/mixup/input_mixup/{cifar10_im100_im_alpha10.yaml, cifar10_im50_im_alpha10.yaml, cifar100_im100_im_alpha10.yaml, cifar100_im50_im_alpha10.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      28.55      |     21.83      |      60.10       |      55.25      |
| <details><summary>Manifold mixup</summary><ol><li>Introduction: <ul><li>Manifold mixup encourages neural networks to predict less confidently on interpolations of hidden representations. We apply manifold mixup on only one layer in our experiments. You can see the section `Mixup training` in our [paper]() for more details. </li><li>The manifold mixup paper link: [Verma et al., ICML 2019](https://arxiv.org/abs/1806.05236).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/mixup/manifold_mixup/{cifar10_im100_mm_alpha10.yaml, cifar10_im50_mm_alpha10.yaml, cifar100_im100_mm_alpha10.yaml, cifar100_im50_mm_alpha10.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |    **27.88**    |   **21.68**    |      60.53       |      56.76      |
| <details><summary>Remix</summary><ol><li>Introduction: <ul><li>Remix assigns the label in favor of the minority class by providing a disproportionately higher weight to the minority class. </li><li>The remix paper link: [Chou et al., ECCV 2020 workshop](https://arxiv.org/abs/1710.09412).</li></ul></li> <br/> <li>CONFIG:<ul><li>config/cui_cifar/mixup/remix/{cifar10_im100_remix_alpha10.yaml, cifar10_im50_remix_alpha10.yaml, cifar100_remix100_im_alpha10.yaml, cifar100_im50_remix_alpha10.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      29.50      |     23.44      |    **59.13**     |    **54.62**    |

### Two-stage training

##### DRW

| Datasets                                                     | CIFAR-10-LT-100 | CIFAR-10-LT-50 | CIFAR-100-LT-100 | CIFAR-100-LT-50 |
| ------------------------------------------------------------ | :-------------: | :------------: | :--------------: | :-------------: |
| <details><summary>Baseline</summary> <ol><li>CONFIG (from left to right):<ul>          <li>config/cui_cifar/baseline/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.12      |     24.81      |      61.76       |      57.65      |

##### DRS

| Datasets                                                     | CIFAR-10-LT-100 | CIFAR-10-LT-50 | CIFAR-100-LT-100 | CIFAR-100-LT-50 |
| ------------------------------------------------------------ | :-------------: | :------------: | :--------------: | :-------------: |
| <details><summary>Baseline</summary> <ol><li>CONFIG (from left to right):<ul>          <li>config/cui_cifar/baseline/{cifar10_im100.yaml, cifar10_im50.yaml, cifar100_im100.yaml, cifar100_im50.yaml}</li></ul>      </li><br/>      <li>Running commands:<ul><li>bash data_parallel_train.sh CONFIG GPU</li></ul> </li>      </ol></details> |      30.12      |     24.81      |      61.76       |      57.65      |

### More results are coming soon.